{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Review Time vs Retention and Rating\n",
    "\n",
    "This notebook explores how real review durations (ms) correlate with FSRS-estimated retrievability and the ratings provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import fsrs_engine\n",
    "from anki_utils import (\n",
    "    START_DATE,\n",
    "    get_review_history_stats,\n",
    "    infer_review_weights,\n",
    "    load_anki_history,\n",
    ")\n",
    "from simulate_fsrs import RustOptimizer, _load_initial_state, run_simulation\n",
    "from simulation_config import SeededData, SimulationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared configuration\n",
    "REPEATS = int(os.getenv(\"REPEATS\", \"5\"))\n",
    "N_DAYS = int(os.getenv(\"N_DAYS\", \"180\"))\n",
    "SEED_HISTORY = os.getenv(\"ANKI_COLLECTION\", \"../collection.anki2\")\n",
    "DECK_CONFIG = os.getenv(\"DECK_CONFIG\", \"GoProblems\")\n",
    "\n",
    "if SEED_HISTORY and not os.path.exists(SEED_HISTORY):\n",
    "    SEED_HISTORY = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and FSRS Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading history for {DECK_CONFIG}...\")\n",
    "logs_data, last_rev = load_anki_history(SEED_HISTORY, deck_config_name=DECK_CONFIG)\n",
    "\n",
    "all_days = (\n",
    "    (logs_data.review_timestamps - np.datetime64(START_DATE)) / np.timedelta64(1, \"D\")\n",
    ").astype(np.int32)\n",
    "\n",
    "print(f\"Fitting FSRS-6 parameters on {len(logs_data.ratings)} reviews...\")\n",
    "optimizer = RustOptimizer(logs_data.card_ids, logs_data.ratings, all_days)\n",
    "fitted_params = tuple(optimizer.compute_optimal_parameters())\n",
    "print(f\"Fitted params: {fitted_params}\")\n",
    "\n",
    "# Pre-calculate card states for seeding\n",
    "print(\"Pre-calculating initial card states (replaying history)...\")\n",
    "deck_t, deck_s, logs_data, current_date = _load_initial_state(\n",
    "    fitted_params, fitted_params, None, SEED_HISTORY, DECK_CONFIG, None\n",
    ")\n",
    "seeded_data = SeededData(\n",
    "    logs=logs_data,\n",
    "    last_rev=current_date - timedelta(days=1),\n",
    "    true_cards=deck_t,\n",
    "    sys_cards=deck_s,\n",
    ")\n",
    "print(\"Seeding complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_review_history_stats(logs_data, fitted_params)\n",
    "df = pd.DataFrame(stats)\n",
    "df = df.dropna(subset=[\"duration\"])\n",
    "assert (df[\"duration\"] == 0).sum() == 0\n",
    "# Convert duration from ms to seconds for easier reading\n",
    "df[\"duration_sec\"] = df[\"duration\"] / 1000.0\n",
    "df[\"is_new_card\"] = df[\"stability\"] == 0\n",
    "mean_duration_sec = df[\"duration_sec\"].mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anki_utils import (\n",
    "    calculate_expected_d0,\n",
    ")\n",
    "\n",
    "# Baseline features for new cards (matches get_review_history_stats)\n",
    "weights = infer_review_weights(logs_data)\n",
    "w_first = weights.first\n",
    "prob_first_success = 1.0 - w_first[0]\n",
    "expected_d0 = calculate_expected_d0(w_first, fitted_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Time Estimation\n",
    "Review time is not constant; it can be more expensive to review certain types of cards.\n",
    "\n",
    "We fit a linear model to give us more accurate estimates of review time. This will use the card state and FSRS model output (i.e. new/review, stability, difficulty and retention) alongside the rating given to estimate review time.\n",
    "\n",
    "We use cross-validation and an L1 penalty term to check if any features are not useful for generalisably forecasting review time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "features = [\"is_new_card\", \"retention\", \"rating\", \"stability\", \"difficulty\"]\n",
    "target = \"duration_sec\"\n",
    "features_matrix = df[features].values\n",
    "target_values = df[target].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features_matrix)\n",
    "\n",
    "# Fit Lasso with Cross-Validation to select alpha\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_scaled, target_values)\n",
    "\n",
    "print(f\"Optimal Alpha: {lasso.alpha_:.6f}\")\n",
    "print(f\"R-squared: {lasso.score(X_scaled, target_values):.4f}\")\n",
    "print(\"--- Coefficients ---\")\n",
    "for feat, coef in zip(features, lasso.coef_, strict=False):\n",
    "    print(f\"{feat}: {coef:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(features, lasso.coef_)\n",
    "plt.title(\"Lasso Coefficients (Standardized Features)\")\n",
    "plt.ylabel(\"Coefficient Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Significance of Review Time Factors\n",
    "While the Lasso model above shows the weight of each factor, we can use Ordinary Least Squares (OLS) to determine the \"statistical significance\" (p-values). \n",
    "\n",
    "**For non-experts:** A p-value (P>|t|) less than 0.05 indicates that we are 95% confident the factor has a real effect on review time and isn't just random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant (intercept) to the features\n",
    "X_ols = sm.add_constant(X_scaled)\n",
    "ols_time = sm.OLS(target_values, X_ols).fit()\n",
    "\n",
    "print(ols_time.summary(xname=[\"const\"] + features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Size of Impact\n",
    "To make this intuitive, let's look at how much the predicted review time changes for different types of reviews compared to the **Constant Baseline** (the average of all reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_series = df[features].mean()\n",
    "\n",
    "\n",
    "def create_scenario(**kwargs):\n",
    "    s = mean_series.copy()\n",
    "    for k, v in kwargs.items():\n",
    "        s[k] = v\n",
    "    return s.values\n",
    "\n",
    "\n",
    "scenarios = {\n",
    "    \"Average (Baseline)\": mean_series.values,\n",
    "    \"Rating: Easy\": create_scenario(rating=4),\n",
    "    \"Rating: Again\": create_scenario(rating=1),\n",
    "    \"New Card\": create_scenario(is_new_card=1, stability=0, difficulty=expected_d0),\n",
    "    \"Stable Card\": create_scenario(stability=df[\"stability\"].quantile(0.9)),\n",
    "}\n",
    "\n",
    "scenario_names = list(scenarios.keys())\n",
    "scenario_feat_vals = np.array(list(scenarios.values()))\n",
    "scenario_feat_scaled = scaler.transform(scenario_feat_vals)\n",
    "predictions = lasso.predict(scenario_feat_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = [\"gray\", \"green\", \"red\", \"orange\", \"blue\"]\n",
    "plt.bar(scenario_names, predictions, color=colors, alpha=0.7)\n",
    "plt.axhline(\n",
    "    mean_duration_sec,\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Constant Baseline ({mean_duration_sec:.2f}s)\",\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Predicted Review Time (seconds)\")\n",
    "plt.title(\"Impact of Card Factors on Predicted Review Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"An 'Easy' review is predicted to take {predictions[1]:.2f}s, \"\n",
    "    f\"while an 'Again' review takes {predictions[2]:.2f}s.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02b994",
   "metadata": {},
   "source": [
    "## Grade Prediction Model\n",
    "\n",
    "The FSRS model estimates success/failure of a review. However, success can have various \"degrees\" (i.e. rating hard, good, easy). Current simulations assume this distribution is uniform across all review cards regardless of card specifics.\n",
    "\n",
    "That is not true. We use an `OrderedModel` to determine the probability distribution over rating (Hard, Good, Easy) for a given review given that a review was successful (retrieved).\n",
    "\n",
    "At simulation time we can sample from the distrubiton given by this `OrderedModel` rather than assuming the same probability distrubiton for all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce1a9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for successful reviews (Rating > 1)\n",
    "df_success = df[df[\"rating\"] > 1].copy()\n",
    "\n",
    "# OrderedModel needs at least 2 unique categories and enough data\n",
    "if len(df_success) >= 10 and df_success[\"rating\"].nunique() > 1:\n",
    "    # Features for grade prediction\n",
    "    grade_features = [\"is_new_card\", \"retention\", \"stability\", \"difficulty\"]\n",
    "    X_grade = df_success[grade_features].values\n",
    "    y_grade = df_success[\"rating\"].values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler_grade = StandardScaler()\n",
    "    X_grade_scaled = scaler_grade.fit_transform(X_grade)\n",
    "\n",
    "    # Fit OrderedModel\n",
    "    try:\n",
    "        mod_prob = OrderedModel(y_grade, X_grade_scaled, distr=\"probit\")\n",
    "        res_prob = mod_prob.fit(method=\"bfgs\", disp=False)\n",
    "        print(res_prob.summary())\n",
    "        HAS_GRADE_MODEL = True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fit OrderedModel: {e}\")\n",
    "        HAS_GRADE_MODEL = False\n",
    "else:\n",
    "    print(\"Insufficient data for OrderedModel, falling back to simple weights.\")\n",
    "    HAS_GRADE_MODEL = False\n",
    "\n",
    "\n",
    "def predict_grades_batch(features_list, weights, count):\n",
    "    if HAS_GRADE_MODEL and len(features_list) > 0:\n",
    "        x_features = np.array(features_list)\n",
    "        x_scaled = scaler_grade.transform(x_features)\n",
    "        probs_all = res_prob.predict(x_scaled)\n",
    "\n",
    "        # Vectorized weighted random choice\n",
    "        cum_probs = probs_all.cumsum(axis=1)\n",
    "        r_vals = np.random.rand(len(probs_all), 1)\n",
    "        choice_indices = (cum_probs < r_vals).sum(axis=1)\n",
    "        choice_indices = np.minimum(choice_indices, len(res_prob.model.labels) - 1)\n",
    "\n",
    "        return res_prob.model.labels[choice_indices].tolist()\n",
    "    else:\n",
    "        # Fallback to simple success weights\n",
    "        return np.random.choice([2, 3, 4], size=count, p=weights.success).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Significance of Grade Distribution\n",
    "The `OrderedModel` summary above shows the significance of each factor on the grade given (for successful reviews).\n",
    "\n",
    "To visualize this, let's look at how the predicted probability of each grade (Hard, Good, Easy) changes as **Retention** increases, while keeping other factors constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GRADE_MODEL:\n",
    "    # 1. Calculate Observed Proportions (Underlay)\n",
    "    num_buckets = 6\n",
    "    df_success[\"ret_bucket\"] = pd.cut(\n",
    "        df_success[\"retention\"], bins=np.linspace(0.7, 1.0, num_buckets + 1)\n",
    "    )\n",
    "    obs_counts = (\n",
    "        df_success.groupby([\"ret_bucket\", \"rating\"], observed=False)\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    obs_props = obs_counts.div(obs_counts.sum(axis=1), axis=0)\n",
    "    bin_mids = np.array([b.mid for b in obs_props.index])\n",
    "\n",
    "    # 2. Model Prediction over range\n",
    "    mean_vals = X_grade.mean(axis=0)\n",
    "    ret_range = np.linspace(0.7, 1.0, 100)\n",
    "\n",
    "    probs_list = []\n",
    "    for r in ret_range:\n",
    "        feat = mean_vals.copy()\n",
    "        feat[1] = r\n",
    "        feat_scaled = scaler_grade.transform([feat])\n",
    "        probs = res_prob.predict(feat_scaled)[0]\n",
    "        probs_list.append(probs)\n",
    "\n",
    "    probs_arr = np.array(probs_list)\n",
    "    labels = res_prob.model.labels  # [2, 3, 4]\n",
    "    label_names = {2: \"Hard\", 3: \"Good\", 4: \"Easy\"}\n",
    "    colors = [\"#ff9999\", \"#66b3ff\", \"#99ff99\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    bucket_width = (1.0 - 0.7) / num_buckets\n",
    "    bar_width = bucket_width / 4\n",
    "    offsets = [-bar_width, 0, bar_width]\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.bar(\n",
    "            bin_mids + offsets[i],\n",
    "            obs_props[label],\n",
    "            width=bar_width,\n",
    "            alpha=0.3,\n",
    "            color=colors[i],\n",
    "            edgecolor=colors[i],\n",
    "            label=f\"Observed {label_names.get(label, label)}\",\n",
    "        )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.plot(\n",
    "            ret_range,\n",
    "            probs_arr[:, i],\n",
    "            color=colors[i],\n",
    "            linewidth=2.5,\n",
    "            label=f\"Model {label_names.get(label, label)}\",\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Retention (FSRS Estimate)\")\n",
    "    plt.ylabel(\"Probability / Proportion\")\n",
    "    plt.title(\"Grade Distribution vs. Retention (Grouped Observed vs. Model)\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Grade model not available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Optimal Retention\n",
    "Current optimal retention assumes all reviews are equal. We can now estimate the true cost (in time) of a review.\n",
    "\n",
    "Let's run simulations using these estimates for review times based on card state at time of review to see the impact this has on optimal retention.\n",
    "\n",
    "Note these simulations are also applying the post-processing discussed in the [Optimal Retention notebook](notebooks/Optimal_Retention.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_time_estimator(deck, indices, date, params, ratings):\n",
    "    cur_stabs = deck.current_stabilities[indices]\n",
    "    cur_last_revs = deck.current_last_reviews[indices]\n",
    "    cur_diffs = deck.current_difficulties[indices]\n",
    "\n",
    "    new_mask = np.isnat(cur_last_revs)\n",
    "    rev_mask = ~new_mask\n",
    "\n",
    "    rets = np.zeros(len(indices))\n",
    "    if np.any(rev_mask):\n",
    "        elapsed = (np.datetime64(date) - cur_last_revs[rev_mask]) / np.timedelta64(\n",
    "            1, \"D\"\n",
    "        )\n",
    "        rets[rev_mask] = fsrs_engine.predict_retrievability(\n",
    "            cur_stabs[rev_mask], elapsed, params\n",
    "        )\n",
    "    if np.any(new_mask):\n",
    "        rets[new_mask] = prob_first_success\n",
    "\n",
    "    feat_matrix = np.column_stack(\n",
    "        [\n",
    "            new_mask.astype(int),\n",
    "            rets,\n",
    "            ratings,\n",
    "            cur_stabs,\n",
    "            np.where(new_mask, expected_d0, cur_diffs),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    features_scaled = scaler.transform(feat_matrix)\n",
    "    pred_secs = lasso.predict(features_scaled)\n",
    "    return np.maximum(0.5, pred_secs).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_rating_estimator(deck, indices, date, params):\n",
    "    count = len(indices)\n",
    "    results = np.zeros(count, dtype=np.int8)\n",
    "\n",
    "    cur_stabs = deck.current_stabilities[indices]\n",
    "    cur_last_revs = deck.current_last_reviews[indices]\n",
    "    cur_diffs = deck.current_difficulties[indices]\n",
    "\n",
    "    new_mask = np.isnat(cur_last_revs)\n",
    "    rev_mask = ~new_mask\n",
    "\n",
    "    if np.any(new_mask):\n",
    "        results[new_mask] = np.random.choice(\n",
    "            [1, 2, 3, 4], size=np.sum(new_mask), p=weights.first\n",
    "        )\n",
    "\n",
    "    if np.any(rev_mask):\n",
    "        n_rev = np.sum(rev_mask)\n",
    "        elapsed = (np.datetime64(date) - cur_last_revs[rev_mask]) / np.timedelta64(\n",
    "            1, \"D\"\n",
    "        )\n",
    "        rets = fsrs_engine.predict_retrievability(cur_stabs[rev_mask], elapsed, params)\n",
    "        success = np.random.random(n_rev) < rets\n",
    "\n",
    "        results_rev = np.ones(n_rev, dtype=np.int8)\n",
    "        n_success = np.sum(success)\n",
    "        if n_success > 0:\n",
    "            success_features = np.column_stack(\n",
    "                [\n",
    "                    np.zeros(n_success),\n",
    "                    rets[success],\n",
    "                    cur_stabs[rev_mask][success],\n",
    "                    cur_diffs[rev_mask][success],\n",
    "                ]\n",
    "            )\n",
    "            results_rev[success] = predict_grades_batch(\n",
    "                success_features, weights, n_success\n",
    "            )\n",
    "\n",
    "        results[rev_mask] = results_rev\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_budget_sec = 10 * 60\n",
    "retentions = [\n",
    "    0.1,\n",
    "    0.3,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.7,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.85,\n",
    "    0.9,\n",
    "    0.925,\n",
    "    0.95,\n",
    "    0.975,\n",
    "]\n",
    "\n",
    "\n",
    "def constant_time_estimator(deck, indices, date, params, ratings):\n",
    "    return np.full(len(indices), mean_duration_sec, dtype=np.float32)\n",
    "\n",
    "\n",
    "results_constant = []\n",
    "results_lasso = []\n",
    "results_advanced = []\n",
    "\n",
    "for ret in tqdm(retentions, desc=\"Simulating\"):\n",
    "    # 1. Constant Time Estimator\n",
    "    config_const = SimulationConfig(\n",
    "        n_days=N_DAYS,\n",
    "        retention=str(ret),\n",
    "        review_limit=None,\n",
    "        new_limit=None,\n",
    "        time_limit=daily_budget_sec,\n",
    "        time_estimator=constant_time_estimator,\n",
    "        verbose=False,\n",
    "        compute_final_params=False,\n",
    "        return_logs=False,\n",
    "    )\n",
    "    _, _, metrics_const = run_simulation(\n",
    "        config_const,\n",
    "        seeded_data=seeded_data,\n",
    "        ground_truth=fitted_params,\n",
    "        initial_params=fitted_params,\n",
    "    )\n",
    "    results_constant.append(metrics_const)\n",
    "\n",
    "    # 2. Lasso Model Time Estimator (Default Rating)\n",
    "    config_lasso = SimulationConfig(\n",
    "        n_days=N_DAYS,\n",
    "        retention=str(ret),\n",
    "        review_limit=None,\n",
    "        new_limit=None,\n",
    "        time_limit=daily_budget_sec,\n",
    "        time_estimator=lasso_time_estimator,\n",
    "        verbose=False,\n",
    "        compute_final_params=False,\n",
    "        return_logs=False,\n",
    "    )\n",
    "    _, _, metrics_lasso = run_simulation(\n",
    "        config_lasso,\n",
    "        seeded_data=seeded_data,\n",
    "        ground_truth=fitted_params,\n",
    "        initial_params=fitted_params,\n",
    "    )\n",
    "    results_lasso.append(metrics_lasso)\n",
    "\n",
    "    # 3. Advanced Estimator (OrderedModel Grade + Lasso Duration)\n",
    "    config_adv = SimulationConfig(\n",
    "        n_days=N_DAYS,\n",
    "        retention=str(ret),\n",
    "        review_limit=None,\n",
    "        new_limit=None,\n",
    "        time_limit=daily_budget_sec,\n",
    "        time_estimator=lasso_time_estimator,\n",
    "        rating_estimator=advanced_rating_estimator,\n",
    "        verbose=False,\n",
    "        compute_final_params=False,\n",
    "        return_logs=False,\n",
    "    )\n",
    "    _, _, metrics_adv = run_simulation(\n",
    "        config_adv,\n",
    "        seeded_data=seeded_data,\n",
    "        ground_truth=fitted_params,\n",
    "        initial_params=fitted_params,\n",
    "    )\n",
    "    results_advanced.append(metrics_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine N_max (baseline population size)\n",
    "max_cards = max(\n",
    "    m[\"card_count\"] for m in results_lasso + results_constant + results_advanced\n",
    ")\n",
    "\n",
    "# 2. Baseline retention for unseen cards\n",
    "r_baseline = prob_first_success\n",
    "\n",
    "\n",
    "def get_adjusted_total(metrics_list, max_n, r_base):\n",
    "    adjusted = []\n",
    "    for m in metrics_list:\n",
    "        # Current sum of retention for studied cards\n",
    "        current_total = m[\"total_retention\"]\n",
    "        # Add hypothetical retention for cards we \"could have\" studied\n",
    "        missing_cards = max_n - m[\"card_count\"]\n",
    "        # Ensure we don't subtract if a run somehow exceeded the max (unlikely)\n",
    "        padding = max(0, missing_cards) * r_base\n",
    "        adjusted.append(current_total + padding)\n",
    "    return adjusted\n",
    "\n",
    "\n",
    "adj_results_constant = get_adjusted_total(results_constant, max_cards, r_baseline)\n",
    "adj_results_lasso = get_adjusted_total(results_lasso, max_cards, r_baseline)\n",
    "adj_results_advanced = get_adjusted_total(results_advanced, max_cards, r_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(\n",
    "    retentions, adj_results_constant, marker=\"o\", label=\"Constant Time Estimator (Mean)\"\n",
    ")\n",
    "plt.plot(\n",
    "    retentions,\n",
    "    adj_results_lasso,\n",
    "    marker=\"o\",\n",
    "    color=\"red\",\n",
    "    label=\"Lasso Model Time Estimator\",\n",
    ")\n",
    "plt.plot(\n",
    "    retentions,\n",
    "    adj_results_advanced,\n",
    "    marker=\"o\",\n",
    "    color=\"green\",\n",
    "    label=\"OrderedModel Grade + Lasso Duration\",\n",
    ")\n",
    "\n",
    "plt.title(f\"Optimal Retention Comparison ({DECK_CONFIG})\")\n",
    "plt.xlabel(\"Target Retention\")\n",
    "plt.ylabel(\"Total Recall (sum of retrievability across deck)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}